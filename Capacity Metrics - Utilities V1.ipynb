{"cells":[{"cell_type":"code","source":["!pip install semantic-link-labs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6693199Z","session_start_time":"2026-02-04T05:46:15.670222Z","execution_start_time":"2026-02-04T05:49:18.2078933Z","execution_finish_time":"2026-02-04T05:49:34.4956124Z","parent_msg_id":"8ec84e93-de59-4dff-af7b-fa11e6667004"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting semantic-link-labs\n  Downloading semantic_link_labs-0.12.10-py3-none-any.whl.metadata (28 kB)\nCollecting semantic-link-sempy>=0.13.0 (from semantic-link-labs)\n  Downloading semantic_link_sempy-0.13.0-py3-none-any.whl.metadata (15 kB)\nCollecting anytree (from semantic-link-labs)\n  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\nCollecting polib (from semantic-link-labs)\n  Downloading polib-1.2.0-py2.py3-none-any.whl.metadata (15 kB)\nCollecting jsonpath_ng (from semantic-link-labs)\n  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: clr_loader>=0.2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (0.2.5)\nCollecting fabric-analytics-sdk<1.0.0,>=0.0.2 (from fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs)\n  Downloading fabric_analytics_sdk-0.0.3.post1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: graphviz>=0.20.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (0.20.1)\nRequirement already satisfied: azure-storage-blob>=12.18.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (12.22.0)\nRequirement already satisfied: azure-core>=1.29.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (1.30.2)\nCollecting azure-keyvault-secrets>=4.7.0 (from semantic-link-sempy>=0.13.0->semantic-link-labs)\n  Downloading azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: azure-storage-file-datalake>=12.12.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (12.16.0)\nRequirement already satisfied: pyarrow>=12.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (14.0.2)\nRequirement already satisfied: pythonnet>=3.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (3.0.1)\nRequirement already satisfied: scikit_learn>=1.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (1.2.2)\nRequirement already satisfied: setuptools>=68.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (68.2.2)\nRequirement already satisfied: tqdm>=4.65.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (4.65.0)\nRequirement already satisfied: rich>=13.3.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (13.3.5)\nRequirement already satisfied: regex>=2023.8.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (2023.10.3)\nRequirement already satisfied: pandas>=1.5.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (2.1.4)\nRequirement already satisfied: pyjwt>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (2.4.0)\nRequirement already satisfied: pyspark>=3.4.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (3.5.1.5.4.20240407)\nRequirement already satisfied: requests>=2.31.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (2.31.0)\nRequirement already satisfied: aiohttp>=3.8.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (3.9.3)\nRequirement already satisfied: IPython>=8.14.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (8.20.0)\nRequirement already satisfied: tenacity>=8.2.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.13.0->semantic-link-labs) (8.2.3)\nRequirement already satisfied: ply in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jsonpath_ng->semantic-link-labs) (3.11)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.13.0->semantic-link-labs) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.13.0->semantic-link-labs) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.9.3)\nRequirement already satisfied: six>=1.11.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core>=1.29.4->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.16.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core>=1.29.4->semantic-link-sempy>=0.13.0->semantic-link-labs) (4.9.0)\nRequirement already satisfied: isodate>=0.6.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.6.1)\nCollecting azure-core>=1.29.4 (from semantic-link-sempy>=0.13.0->semantic-link-labs)\n  Downloading azure_core-1.38.0-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: cryptography>=2.1.4 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-storage-blob>=12.18.3->semantic-link-sempy>=0.13.0->semantic-link-labs) (42.0.2)\nRequirement already satisfied: cffi>=1.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from clr_loader>=0.2.5->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.16.0)\nRequirement already satisfied: azure-identity in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-sdk<1.0.0,>=0.0.2->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.15.0)\nCollecting fabric-analytics-notebook-plugin (from fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs)\n  Downloading fabric_analytics_notebook_plugin-0.0.3-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: decorator in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.15.1)\nRequirement already satisfied: stack-data in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.2.0)\nRequirement already satisfied: traitlets>=5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (5.7.1)\nRequirement already satisfied: pexpect>4.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (4.8.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.13.0->semantic-link-labs) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.13.0->semantic-link-labs) (2023.3)\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.4.1->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.10.9.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (2024.2.2)\nRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit_learn>=1.2.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit_learn>=1.2.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit_learn>=1.2.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.2.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.13->clr_loader>=0.2.5->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.21)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jedi>=0.16->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.8.3)\nRequirement already satisfied: mdurl~=0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.1.0)\nRequirement already satisfied: ptyprocess>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pexpect>4.3->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.7.0)\nRequirement already satisfied: wcwidth in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.2.5)\nRequirement already satisfied: msal<2.0.0,>=1.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk<1.0.0,>=0.0.2->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.25.0)\nRequirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk<1.0.0,>=0.0.2->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.0.0)\nRequirement already satisfied: psutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (5.9.0)\nRequirement already satisfied: synapseml-utils in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.1.6.post2)\nRequirement already satisfied: executing in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.8.3)\nRequirement already satisfied: asttokens in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.0.5)\nRequirement already satisfied: pure-eval in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.2.2)\nRequirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->fabric-analytics-sdk<1.0.0,>=0.0.2->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (2.3.0)\nRequirement already satisfied: fluent-logger in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from synapseml-utils->fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.10.0)\nRequirement already satisfied: gson in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from synapseml-utils->fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (0.0.4)\nRequirement already satisfied: msgpack>1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fluent-logger->synapseml-utils->fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]<1.0.0,>=0.0.2->semantic-link-sempy>=0.13.0->semantic-link-labs) (1.0.3)\nDownloading semantic_link_labs-0.12.10-py3-none-any.whl (819 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m819.4/819.4 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading semantic_link_sempy-0.13.0-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\nDownloading polib-1.2.0-py2.py3-none-any.whl (20 kB)\nDownloading azure_keyvault_secrets-4.10.0-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_core-1.38.0-py3-none-any.whl (217 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m217.8/217.8 kB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fabric_analytics_sdk-0.0.3.post1-py3-none-any.whl (43 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.0/43.0 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fabric_analytics_notebook_plugin-0.0.3-py3-none-any.whl (24 kB)\nInstalling collected packages: polib, jsonpath_ng, anytree, azure-core, azure-keyvault-secrets, fabric-analytics-sdk, fabric-analytics-notebook-plugin, semantic-link-sempy, semantic-link-labs\n  Attempting uninstall: azure-core\n    Found existing installation: azure-core 1.30.2\n    Uninstalling azure-core-1.30.2:\n      Successfully uninstalled azure-core-1.30.2\n  Attempting uninstall: semantic-link-sempy\n    Found existing installation: semantic-link-sempy 0.11.0\n    Uninstalling semantic-link-sempy-0.11.0:\n      Successfully uninstalled semantic-link-sempy-0.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anytree-2.13.0 azure-core-2024.9.1 azure-keyvault-secrets-4.10.0 fabric-analytics-notebook-plugin-0.0.3 fabric-analytics-sdk-0.0.3.post1 jsonpath_ng-1.7.0 polib-1.2.0 semantic-link-labs-0.12.10 semantic-link-sempy-0.13.0\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"65320259-f43e-4e53-9643-4cf9ba79d8c7"},{"cell_type":"code","source":["spark.conf.set(\"spark.sql.legacy.timeParserPolicy\" , \"LEGACY\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6731444Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:34.4975988Z","execution_finish_time":"2026-02-04T05:49:34.8314163Z","parent_msg_id":"9a57baea-3eb1-43c3-ae5f-1ba785a206ba"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d6dfc9fe-41c5-47f0-b19c-bac4b48d273b"},{"cell_type":"markdown","source":["**Variables for service principal and the Capacity Metrics Workspace Name and Dataset Name**"],"metadata":{},"id":"c3dc43f9"},{"cell_type":"code","source":["capacity_metrics_ws_name = \"Microsoft Fabric Capacity Metrics\"\n","capacity_metrics_target_dataset_name = \"Fabric Capacity Metrics\"\n","\n","tenant_id = \"\"\n","subscriptionid = \"\"\n","client_id = \"\"\n","client_secret = \"\""],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6750414Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:34.8331006Z","execution_finish_time":"2026-02-04T05:49:35.1372112Z","parent_msg_id":"394dbc5e-84e9-4ee8-ae2a-5c8403c56bf5"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7f0725b9-9bd0-46b3-88a7-2a5dc472f373"},{"cell_type":"markdown","source":["**Helper functions to format column names**"],"metadata":{},"id":"04cd693c"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","import pandas as pd\n","import re\n","from pyspark.sql.functions import col\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import lit\n","from pyspark.sql.types import StringType\n","\n","def sanitize_column_name(name: str) -> str:\n","    # Replace invalid characters with underscores, keep alphanumeric and underscores\n","    sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', name)\n","    # Ensure name doesn't start with a digit\n","    if sanitized and sanitized[0].isdigit():\n","        sanitized = f\"col_{sanitized}\"\n","    return sanitized.lower()\n","\n","def format_column_names(df):   \n","    formatted_col_list = [sanitize_column_name(col_name) for col_name in df.columns]\n","    formatted_col_list = [col.rstrip('_') for col in formatted_col_list]\n","    df = df.toDF(*formatted_col_list)\n","    return df\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6768046Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:35.1392082Z","execution_finish_time":"2026-02-04T05:49:38.3870411Z","parent_msg_id":"ead20945-584f-409a-a80e-01493dcda6ce"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 6, Finished, Available, Finished)"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a17c7582"},{"cell_type":"markdown","source":["**1.0 The code fetches service principal from Azure Key Vault.**\n","\n","It gets an access token once, then uses it to call the Key Vault REST API for each secret.\n","It includes retry logic for transient errors and returns a dictionary of secret names and values."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"16b85e73-d7fd-47aa-857b-411cd1a6dac9"},{"cell_type":"code","source":["# Standard library & third-party imports\n","import requests                              # HTTP client for calling REST APIs (used here for Key Vault)\n","import json                                  # JSON serialization/deserialization\n","import os                                    # OS utilities (env vars, paths)\n","from datetime import datetime, timedelta     # Date/time utilities\n","import pandas as pd                          # Data processing (not used in this snippet)\n","import sempy.fabric as fabric                # Microsoft Fabric / semantic tooling (imported but not used below)\n","from typing import Iterable, Dict, Optional  # Type hints\n","\n","\n","\n","# -----------------------------\n","# Key Vault Secret Fetch Helper\n","# -----------------------------\n","def get_kv_secrets(\n","    kv_name: str,\n","    secret_names: Iterable[str],\n","    api_version: str = \"7.3\",\n","    timeout_sec: int = 30,\n","    max_retries: int = 2,\n","    backoff_sec: float = 0.5,\n",") -> Dict[str, Optional[str]]:\n","    \"\"\"\n","    Fetch one or more secrets from Azure Key Vault using the notebook's execution identity.\n","    Uses notebookutils.credentials.getToken('https://vault.azure.net') once and reuses it\n","    for all secret calls.\n","\n","    Args:\n","        kv_name:                Key Vault name (e.g., 'kvrbacpocsk')\n","        secret_names:           Iterable of secret names to fetch (e.g., ['TENANTID', 'subscriptionid'])\n","        api_version:            Key Vault API version (default '7.3')\n","        timeout_sec:            Per-call HTTP timeout\n","        max_retries:            Number of retries on transient 5xx/429 errors\n","        backoff_sec:            Backoff seconds between retries\n","\n","    Returns:\n","        Dict mapping secret name -> secret value (or None if not found/failed).\n","    \"\"\"\n","    # 1) Acquire a bearer token for Key Vault using the Spark notebook identity.\n","    #    This relies on Synapse/Fabric's notebookutils to obtain an AAD token scoped to Key Vault.\n","    token = notebookutils.credentials.getToken(\"https://vault.azure.net\")\n","\n","    # Standard auth + content headers; we reuse these for all requests\n","    headers = {\n","        \"Authorization\": f\"Bearer {token}\",\n","        \"Content-Type\": \"application/json\",\n","    }\n","\n","    # Base URL for the target Key Vault (data-plane endpoint)\n","    base_url = f\"https://{kv_name}.vault.azure.net\"\n","\n","    # Small inner helper to GET with basic retry/backoff handling\n","    def _get_with_retry(url: str) -> Optional[dict]:\n","        attempt = 0\n","        while True:\n","            try:\n","                # Make the HTTP GET call with a per-call timeout\n","                resp = requests.get(url, headers=headers, timeout=timeout_sec)\n","\n","                # Retry on common transient statuses (throttling/temporary failures)\n","                if resp.status_code in (429, 500, 502, 503, 504):\n","                    if attempt < max_retries:\n","                        attempt += 1\n","                        import time\n","                        time.sleep(backoff_sec * attempt)  # Linear backoff by attempt count\n","                        continue  # Try again\n","                # Raise for any non-success HTTP status codes (will enter except)\n","                resp.raise_for_status()\n","\n","                # On success, return parsed JSON (Key Vault returns JSON for secrets)\n","                return resp.json()\n","\n","            except requests.exceptions.HTTPError as e:\n","                # If Key Vault returns 404, the secret doesn't exist â€” treat as a clean miss\n","                if getattr(e.response, \"status_code\", None) == 404:\n","                    return None\n","                # For other HTTP errors, propagate the exception up to the caller\n","                raise\n","\n","            except requests.exceptions.RequestException:\n","                # Network or other request-layer issues â€” retry if budget remains\n","                if attempt < max_retries:\n","                    attempt += 1\n","                    import time\n","                    time.sleep(backoff_sec * attempt)\n","                    continue\n","                # Retries exhausted â€” re-raise to signal failure\n","                raise\n","\n","    # Aggregate results: name -> value (or None if not found/failed)\n","    results: Dict[str, Optional[str]] = {}\n","    for name in secret_names:\n","        # Build the per-secret URL; Key Vault \"secrets\" data-plane API\n","        url = f\"{base_url}/secrets/{name}?api-version={api_version}\"\n","\n","        # Attempt to fetch the secret JSON (may be None if 404 or repeated failures)\n","        data = _get_with_retry(url)\n","\n","        # Key Vault returns {\"value\": \"<secret>\"} on success; otherwise None for misses/failures\n","        results[name] = data.get(\"value\") if data else None\n","\n","    # Return a flat dict mapping requested secret names to their values (or None)\n","    return results"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6811906Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:38.3893009Z","execution_finish_time":"2026-02-04T05:49:38.6902243Z","parent_msg_id":"f5f3664b-4dd0-448b-8241-7ba9e5e8c512"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"33e9fa10-a87a-4500-9d80-abeb873b46f1"},{"cell_type":"markdown","source":["**2.0 This code looks up the workspace and dataset IDs for the Fabric Capacity Metrics dataset.**\n","\n","It uses the Fabric client to find the workspace by name, then tries to resolve the dataset ID directly.\n","If that fails, it lists all datasets in the workspace and searches for the requested one.\n","Finally, it returns the workspace name, workspace ID, dataset name, and dataset ID (or None if not found)."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"fdeb0b07-5813-4cb0-9482-3c39ff3a15df"},{"cell_type":"code","source":["import pandas as pd\n","from sempy.fabric import set_service_principal\n","import sempy.fabric as fabric\n","\n","def extract_metrics_workspace_details():\n","\n","    with set_service_principal(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret):\n","\n","        # 1) Resolve workspace by name (case/whitespace normalized)\n","        ws_df = fabric.list_workspaces()\n","        ws_match = ws_df[\n","            ws_df[\"Name\"].str.strip().str.casefold() == capacity_metrics_ws_name.strip().casefold()\n","        ]\n","\n","        if ws_match.empty:\n","            raise ValueError(f\"Workspace '{capacity_metrics_ws_name}' not found.\")\n","        \n","        target_workspace_id = ws_match.iloc[0][\"Id\"]\n","\n","        # 2) List datasets (semantic models) in this workspace\n","        ds_df = fabric.list_items(workspace=target_workspace_id, item_type=\"semanticModel\")\n","\n","        # display(ds_df)\n","\n","        if ds_df is None or ds_df.empty:\n","            raise ValueError(f\"No datasets found in workspace '{capacity_metrics_ws_name}'\")\n","\n","        # Normalize DisplayName for comparison (handles trailing CR/LF)\n","        ds_df[\"NormalizedName\"] = ds_df[\"Display Name\"].str.strip().str.replace(\"\\r\",\"\").str.replace(\"\\n\",\"\").str.casefold()\n","\n","    target_normalized = capacity_metrics_target_dataset_name.strip().casefold()\n","\n","    ds_match = ds_df[ds_df[\"NormalizedName\"] == target_normalized]\n","\n","    if ds_match.empty:\n","        raise ValueError(f\"Dataset '{capacity_metrics_target_dataset_name}' not found in workspace '{capacity_metrics_ws_name}'\")\n","\n","    target_dataset_id = ds_match.iloc[0][\"Id\"]\n","    return capacity_metrics_ws_name, target_workspace_id, capacity_metrics_target_dataset_name, target_dataset_id\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.684026Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:38.6921452Z","execution_finish_time":"2026-02-04T05:49:38.9986319Z","parent_msg_id":"56584f53-1755-4baf-a18e-41365f0236d3"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"18e143a0-5059-42ac-b60e-1204941bb32e"},{"cell_type":"markdown","source":["**3.0 This function triggers a dataset (semantic model) refresh in Fabric/Power BI and then polls the refresh status until it finishes.**\n","\n","It starts the refresh, identifies where to check progress, repeatedly checks the refresh state, and returns the final result (success or failure).\n","If polling never reaches a terminal state, it times out."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"6dca3d0f-bc80-47c6-9efc-50b2c4f8b14a"},{"cell_type":"code","source":["\n","import time\n","import sempy.fabric as fabric\n","\n","def refresh_metrics_dataset(\n","    workspace_id: str,\n","    dataset_id: str,\n","    *,\n","    refresh_type: str = \"Full\",          # \"Full\", \"Calculate\", \"DataOnly\", \"ClearValues\", \"Automatic\", \"Defragment\"\n","    poll_interval_sec: int = 15,         # how often to poll\n","    max_attempts: int = 60               # 15s * 60 = 15 minutes total wait\n",") -> dict:\n","    \"\"\"\n","    Triggers a Power BI/Fabric semantic model (dataset) refresh and polls until terminal state.\n","    Returns the final refresh record (dict). Raises RuntimeError/TimeoutError on issues.\n","    \"\"\"\n","\n","    with set_service_principal(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret):\n","        frc = fabric.FabricRestClient()\n","\n","    # 1) Trigger workspace-scoped refresh\n","\n","    trigger_path = f\"https://api.powerbi.com/v1.0/myorg/datasets/{dataset_id}/refreshes\"\n","    body = {\n","        \"type\": refresh_type\n","    }\n","\n","    resp = frc.post(trigger_path, json=body)\n","    if resp.status_code != 202:\n","        raise RuntimeError(f\"Failed to trigger refresh: {resp.status_code} {resp.text}\")\n","\n","    # 2) Prefer Location header to poll the specific refresh operation\n","    location = resp.headers.get(\"Location\")\n","    request_id = resp.headers.get(\"RequestId\") or resp.headers.get(\"requestid\")\n","\n","    if location:\n","        # Location is a full URL. Convert to a relative path for FabricRestClient (which wants paths).\n","        # Example Location: https://api.powerbi.com/v1.0/myorg/groups/{g}/datasets/{d}/refreshes/{rid}\n","        poll_path = location.split(\".com\")[-1]  # keep everything after domain\n","    elif request_id:\n","        poll_path = f\"/groups/{workspace_id}/datasets/{dataset_id}/refreshes/{request_id}\"\n","    else:\n","        # Fallback: poll the collection and read latest entry\n","        poll_path = f\"/groups/{workspace_id}/datasets/{dataset_id}/refreshes\"\n","\n","    # 3) Poll until terminal status\n","    for attempt in range(max_attempts):\n","        r = frc.get(poll_path)\n","        if r.status_code not in (200, 202):\n","            raise RuntimeError(f\"Polling error: {r.status_code} {r.text}\")\n","\n","        data = r.json()\n","        # If polling a single refresh, JSON is an object; if polling the collection, JSON has 'value'\n","        refresh = data if \"status\" in data else (data.get(\"value\", [{}])[0])\n","        status = (refresh or {}).get(\"status\")\n","        print(f\"[{attempt}] status={status}\")\n","\n","        if status in (\"Completed\", \"Failed\", \"Cancelled\"):\n","            if status == \"Completed\":\n","                print(\"Refresh succeeded.\")\n","            else:\n","                print(f\"Refresh finished with status: {status}\")\n","                # When available, serviceExceptionJson gives richer details\n","                print(refresh.get(\"serviceExceptionJson\") or refresh)\n","            return refresh\n","\n","        time.sleep(poll_interval_sec)\n","\n","    raise TimeoutError(\"Polling timed out. Check refresh history in the Power BI service.\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6857214Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:39.0004633Z","execution_finish_time":"2026-02-04T05:49:39.2696841Z","parent_msg_id":"f7742d54-19df-4373-8628-02172a9f8d47"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"454231f3-55a4-43db-9ae7-eef46524ba8b"},{"cell_type":"markdown","source":["**4.0 This code updates two mashup parameters (DefaultCapacityID and RegionName) on a Fabric/Power BI dataset and can optionally verify that the parameters exist first.**\n","\n","After updating, it can automatically trigger a dataset refresh to apply the new values.\n","It uses the Fabric REST client and raises clear errors if parameter validation or HTTP calls fail."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"ca5638f4-269f-4877-a62a-f1c2458a6cda"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","\n","\n","def update_metrics_dataset(\n","    metrics_workspace_id: str,\n","    metrics_dataset_id: str,\n","    capacity_id_to_extract_data_from: str,\n","    region_name: str,\n","    *,\n","    verify_parameters_exist: bool = True,\n","    trigger_refresh: bool = True,\n",") -> None:\n","    \"\"\"\n","    Update two mashup parameters on a Fabric/Power BI dataset:\n","        - DefaultCapacityID\n","        - RegionName\n","\n","    Then (optionally) trigger a dataset refresh.\n","\n","    Args:\n","        metrics_workspace_id: Workspace (group) GUID containing the dataset\n","        metrics_dataset_id: Dataset GUID (semantic model)\n","        capacity_id_to_extract_data_from: New value for DefaultCapacityID (GUID)\n","        region_name: New value for RegionName (e.g., 'West US 3')\n","        verify_parameters_exist: If True, GET parameters first and fail fast if missing\n","        trigger_refresh: If True, call refresh_metrics_dataset(...) after update\n","\n","    Raises:\n","        RuntimeError / FabricHTTPException on HTTP failures or parameter validation errors.\n","    \"\"\"\n","    with set_service_principal(\n","        tenant_id=tenant_id, client_id=client_id, client_secret=client_secret\n","    ):\n","        client = fabric.FabricRestClient()\n","\n","    # ----------------------------------------------------------------------\n","    # (Optional) Verify the two parameters exist before attempting update\n","    # ----------------------------------------------------------------------\n","    if verify_parameters_exist:\n","        get_params_path = (\n","            \"https://api.powerbi.com/v1.0/myorg/groups/\"\n","            f\"{metrics_workspace_id}/datasets/{metrics_dataset_id}/parameters\"\n","        )\n","        try:\n","            r = client.get(get_params_path)\n","        except FabricHTTPException as ex:\n","            raise RuntimeError(f\"Failed to read dataset parameters: {ex}\") from ex\n","\n","        if r.status_code != 200:\n","            raise RuntimeError(f\"Cannot read parameters: {r.status_code} {r.text}\")\n","\n","        params = r.json().get(\"value\", [])\n","        names = {p.get(\"name\") for p in params}\n","        required = {\"DefaultCapacityID\", \"RegionName\"}\n","        missing = required - names\n","        if missing:\n","            raise RuntimeError(\n","                f\"Dataset is missing required parameters: {sorted(missing)}. \"\n","                f\"Found: {sorted(names)}\"\n","            )\n","\n","    # ----------------------------------------------------------------------\n","    # Build payload and POST UpdateParameters (workspace scoped)\n","    # ----------------------------------------------------------------------\n","    utc_offset = \"0\"\n","    update_path = (\n","        \"https://api.powerbi.com/v1.0/myorg/groups/\"\n","        f\"{metrics_workspace_id}/datasets/{metrics_dataset_id}/Default.UpdateParameters\"\n","    )\n","    payload = {\n","        \"updateDetails\": [\n","            {\"name\": \"DefaultCapacityID\", \"newValue\": str(capacity_id_to_extract_data_from)},\n","            {\"name\": \"RegionName\", \"newValue\": str(region_name)},\n","            {\"name\": \"UTC_offset\", \"newValue\": str(utc_offset)},\n","        ]\n","    }\n","\n","    try:\n","        resp = client.post(update_path, json=payload)\n","    except FabricHTTPException as ex:\n","        raise RuntimeError(f\"Failed to update dataset parameters: {ex}\") from ex\n","\n","    if resp.status_code != 200:\n","        raise RuntimeError(f\"UpdateParameters failed: {resp.status_code} {resp.text}\")\n","\n","    print(\"âœ… Parameters updated successfully.\")\n","\n","    # ----------------------------------------------------------------------\n","    # (Optional) trigger a refresh so new parameter values are applied\n","    # ----------------------------------------------------------------------\n","    if trigger_refresh:\n","        # assumes you have the helper defined as in our earlier step:\n","        #   refresh_metrics_dataset(workspace_id, dataset_id, ...)\n","        try:\n","            print(\"ðŸ”„ Triggering dataset refreshâ€¦\")\n","            _ = refresh_metrics_dataset(\n","                workspace_id=metrics_workspace_id,\n","                dataset_id=metrics_dataset_id,\n","                refresh_type=\"Full\",\n","                poll_interval_sec=15,\n","                max_attempts=60,\n","            )\n","        except Exception:\n","            # Surface but do not hide the successful parameter update\n","            raise RuntimeError(\"Parameters are not valid\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6892343Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:39.2720747Z","execution_finish_time":"2026-02-04T05:49:39.5391884Z","parent_msg_id":"0fb4f449-ce06-484e-9dbc-ac71cb0a8561"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"47dc0a02-94ab-4d31-9eed-56969a0a0107"},{"cell_type":"markdown","source":["**5.0 This code queries a Fabric/Power BI dataset table using DAX, handles pagination, cleans up the returned column names, and writes the results into a Delta table.**\n","\n","It also includes helpers to sanitize column names and compute a default start date by looking up the maximum date from an existing table."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"53ec5833-4b17-48a6-8739-dc7e39201e3c"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","import pandas as pd\n","import re\n","from pyspark.sql.functions import col\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import lit\n","from pyspark.sql.types import StringType\n","\n","def get_table_data(metrics_workspace_id, metrics_dataset_id, table_name, capacity_id):\n","\n","    try:\n","        url = f\"https://api.powerbi.com/v1.0/myorg/groups/{metrics_workspace_id}/datasets/{metrics_dataset_id}/executeQueries\"\n","\n","\n","        if table_name == \"Metrics By Item and Day\":\n","            \n","            start_iso = get_max_or_default(\"metrics_by_item_and_day\", \"metrics_by_item_and_day_datetime\")\n","            print(f\"getting the data from start_iso = {start_iso}\")\n","\n","            # dax_query = {\n","            # \"queries\": [\n","            #     {\n","            #     \"query\": f\"EVALUATE FILTER('{table_name}', '{table_name}'[Date] >= DATEVALUE(\\\"{start_iso}\\\"))  && '{table_name}'[[Capacity Id]] = \\\"{capacity_id}\\\"\"\n","            #     # \"query\": f\"EVALUATE FILTER('{table_name}', '{table_name}'[Date] >= DATEVALUE(\\\"{start_iso}\\\"))\"                \n","            #     }\n","            # ],\n","            # \"serializerSettings\": { \"includeNulls\": True }\n","            # }\n","\n","            dax_query = {\n","                \"queries\": [\n","                    {\n","                        \"query\": f\"\"\"\n","            EVALUATE\n","            FILTER(\n","                'Metrics By Item and Day',\n","                'Metrics By Item and Day'[Date] >= DATEVALUE(\"{start_iso}\")\n","                    && 'Metrics By Item and Day'[Capacity Id] = \"{capacity_id}\"\n","            )\n","            \"\"\"\n","                    }\n","                ]\n","            }\n","\n","        else:\n","            dax_query = {\n","                \"queries\": [{\n","                    \"query\":f\"EVALUATE '{table_name}'\"\n","                }],\n","                \"serializerSettings\": {\"includeNulls\": True}\n","            }\n","\n","        headers = {\n","            \"Content-Type\": \"application/json\",\n","\n","        }\n","\n","        rows = []\n","        continuation_token = None\n","\n","        while True:\n","\n","            query_payload = dax_query.copy()\n","            if continuation_token:\n","                query_payload[\"continuationToken\"] = continuation_token\n","\n","            with set_service_principal(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret): \n","                 fabric_rest_client = fabric.FabricRestClient()\n","\n","            resp = fabric_rest_client.post(url, headers=headers, json=query_payload)\n","            result = resp.json()\n","            # print(result)  \n","            \n","            # Parse rows\n","            new_rows = result['results'][0]['tables'][0]['rows']\n","            rows.extend(new_rows)\n","\n","            # Check for next page\n","            continuation_token = result['results'][0].get('continuationToken')\n","            if not continuation_token:\n","                break\n","\n","        df_metrics = pd.DataFrame(rows)\n","        df_metrics = spark.createDataFrame(df_metrics)\n","        df_metrics = format_column_names(df_metrics)\n","        table_name = \"stg_\" + sanitize_column_name(f'{table_name}')\n","\n","        # Cast each void column to string\n","        void_cols = [col for col, dtype in df_metrics.dtypes if dtype == 'void']\n","        for col in void_cols:\n","            df_metrics = df_metrics.withColumn(col, lit(None).cast(StringType()))\n","        df_metrics.write.option(\"mergeSchema\", \"true\").mode('overwrite').format('delta').saveAsTable(f'{table_name}')\n","\n","    except Exception as e:\n","        print(f\"An unexpected error occurred: {e}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6909688Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:39.5412767Z","execution_finish_time":"2026-02-04T05:49:39.8097307Z","parent_msg_id":"8c764b53-6bc3-479b-a6c9-73c8d625fbd2"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"856c1f3b-f5e1-400a-a0d8-d0c9a45c2f3f"},{"cell_type":"code","source":["import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","import pandas as pd\n","import re\n","from pyspark.sql.functions import col\n","from pyspark.sql import functions as F\n","from pyspark.sql.functions import lit\n","from pyspark.sql.types import StringType\n","from pyspark.sql import functions as F\n","\n","def get_max_or_default(table_name: str, column_name: str, default_value: str = \"1970-01-01\") -> str:\n","    \"\"\"\n","    Returns (max(column) - 7 day) as 'YYYY-MM-DD' from a given table/column.\n","    If the table/column doesn't exist or is empty, returns default_value.\n","    \"\"\"\n","    days_to_extract = 7\n","    # Check if table exists\n","    if spark.catalog.tableExists(table_name):\n","        df = spark.read.table(table_name)\n","\n","        # Check if column exists\n","        if column_name in df.columns:\n","            # Parse column as date (handles date/timestamp/text formats)\n","            parsed_date = F.coalesce(\n","                F.col(column_name).cast(\"date\"),\n","                F.to_date(F.col(column_name), \"yyyy-MM-dd\"),\n","                F.to_date(F.col(column_name), \"MM/dd/yyyy\"),\n","                F.to_date(F.col(column_name), \"yyyy/MM/dd\"),\n","                F.to_timestamp(F.col(column_name), \"yyyy-MM-dd HH:mm:ss\").cast(\"date\"),\n","                F.to_timestamp(F.col(column_name), \"yyyy-MM-dd'T'HH:mm:ss\").cast(\"date\")\n","            )\n","\n","            # Compute max date\n","            max_val = df.select(F.max(parsed_date).alias(\"max_date\")).collect()[0][\"max_date\"]\n","\n","            if max_val is not None:\n","                # Subtract 1 day and return as string\n","                adjusted_date = spark.createDataFrame([(max_val,)], [\"d\"]) \\\n","                                      .select(F.date_sub(F.col(\"d\"), days_to_extract).alias(\"d_sub\")) \\\n","                                      .collect()[0][\"d_sub\"]\n","                return str(adjusted_date)\n","            else:\n","                print(f\"Column '{column_name}' is empty. Using default.\")\n","                return default_value\n","        else:\n","            print(f\"Column '{column_name}' not found in table '{table_name}'. Using default.\")\n","            return default_value\n","    else:\n","        print(f\"Table '{table_name}' does not exist. Using default.\")\n","        return default_value\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.6928444Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:39.8116578Z","execution_finish_time":"2026-02-04T05:49:40.0975057Z","parent_msg_id":"baf51647-013d-40b4-9bd5-f6d919eeddd5"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf5789e2-6702-4706-8a05-09dd32a31a51"},{"cell_type":"markdown","source":["**6.0 This code checks for patterns (integers, floats, dates, timestamps, or booleans) and applies the correct Spark cast.**\n","\n","It also fixes columns that Spark loads as all-null (NullType) by casting them to a real type, usually string or a type from a provided schema."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"d7b0b815-091c-4fbd-aecb-8bfb2571969d"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","from pyspark.sql.types import (\n","    StringType, LongType, DoubleType, BooleanType, DateType, TimestampType, NullType\n",")\n","import re\n","\n","sample_size = 1000\n","\n","def looks_like(patterns, s): return any(re.fullmatch(p, s) for p in patterns)\n","\n","def infer_column_type(series):\n","    date_patterns = [r\"\\d{4}-\\d{2}-\\d{2}\", r\"\\d{2}/\\d{2}/\\d{4}\", r\"\\d{4}/\\d{2}/\\d{2}\"]\n","    ts_patterns   = [r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}\", r\"\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\"]\n","\n","    # -------- Inference helpers --------\n","    def is_int(s): return re.fullmatch(r\"-?\\d+\", s) is not None\n","    def is_float(s): return re.fullmatch(r\"-?\\d+(?:\\.\\d+)?\", s) is not None\n","    def is_bool(s): return s.lower() in {\"true\", \"false\", \"yes\", \"no\", \"0\", \"1\"}\n","\n","    cleaned = series.dropna().astype(str).str.strip()\n","    cleaned = cleaned[cleaned != \"\"]\n","    if cleaned.empty: return \"string\"\n","    if cleaned.apply(is_bool).all(): return \"boolean\"\n","    if cleaned.apply(is_int).all(): return \"long\"\n","    if cleaned.apply(is_float).all(): return \"double\"\n","    if cleaned.apply(lambda s: looks_like(ts_patterns, s)).all(): return \"timestamp\"\n","    if cleaned.apply(lambda s: looks_like(date_patterns, s)).all(): return \"date\"\n","    return \"string\"\n","\n","# -------- Casting helper (strings -> typed) --------\n","def cast_column(col_name, target_type):\n","    c = F.col(col_name)\n","    if target_type == \"boolean\":\n","        return F.when(F.lower(c).isin(\"true\", \"yes\", \"1\"), F.lit(True)) \\\n","                .when(F.lower(c).isin(\"false\", \"no\", \"0\"), F.lit(False)) \\\n","                .otherwise(F.lit(None)).alias(col_name)\n","    elif target_type == \"long\":\n","        return c.cast(LongType()).alias(col_name)\n","    elif target_type == \"double\":\n","        return c.cast(DoubleType()).alias(col_name)\n","    elif target_type == \"date\":\n","        return F.coalesce(F.to_date(c, \"yyyy-MM-dd\"), F.to_date(c, \"MM/dd/yyyy\")).alias(col_name)\n","    elif target_type == \"timestamp\":\n","        return F.coalesce(F.to_timestamp(c, \"yyyy-MM-dd HH:mm:ss\"),\n","                           F.to_timestamp(c, \"yyyy-MM-dd'T'HH:mm:ss\")).alias(col_name)\n","    else:\n","        # string: standardize by trimming whitespace\n","        return F.trim(c).alias(col_name)\n","\n","# -------- VOID/NullType fix: cast all-null cols to explicit types --------\n","def fix_void_columns(df, final_schema_map=None):\n","    exprs = []\n","    for f in df.schema.fields:\n","        if isinstance(f.dataType, NullType):\n","            # If the column exists in final, cast to its type; else cast to STRING\n","            target_dt = final_schema_map.get(f.name, StringType()) if final_schema_map else StringType()\n","            exprs.append(F.col(f.name).cast(target_dt).alias(f.name))\n","        else:\n","            exprs.append(F.col(f.name))\n","    return df.select(*exprs)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7127836Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:40.0996444Z","execution_finish_time":"2026-02-04T05:49:40.4007305Z","parent_msg_id":"dad4eed3-d612-44fb-baaf-ff1a6d0824d4"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0277ab5f-0060-4e84-ba0e-ecccb404bf4b"},{"cell_type":"markdown","source":["**7.0 This code retrieves daily Azure cost data for the current monthâ€”filtered specifically for Microsoft Fabricâ€”by calling the Azure Cost Management API using client-credential authentication.**\n","\n","It formats and normalizes the returned cost fields, converts the data into a clean table, and writes it to a Delta table when Spark is available (otherwise it returns a Pandas DataFrame)."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"37532533-8672-47ad-b48c-6c65d1f4b730"},{"cell_type":"code","source":["\n","import requests\n","import pandas as pd\n","from datetime import datetime, timedelta, timezone\n","from calendar import monthrange\n","\n","# Optional: detect Spark context if you're running in a Fabric/Spark notebook\n","try:\n","    spark  # noqa: F821\n","    _HAS_SPARK = True\n","except NameError:\n","    _HAS_SPARK = False\n","\n","\n","def _aad_client_credentials_token(tenant_id: str, client_id: str, client_secret: str) -> str:\n","    \"\"\"\n","    Acquire an AAD access token using OAuth2 client-credentials (v2 endpoint)\n","    for Azure Resource Manager (Cost Management) APIs.\n","    \"\"\"\n","    token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n","    data = {\n","        \"grant_type\": \"client_credentials\",\n","        \"client_id\": client_id,\n","        \"client_secret\": client_secret,\n","        # v2 requires 'scope', not 'resource'\n","        \"scope\": \"https://management.azure.com/.default\",\n","    }\n","    r = requests.post(token_url, data=data, timeout=60)\n","    try:\n","        r.raise_for_status()\n","    except requests.HTTPError as ex:\n","        raise RuntimeError(f\"Token request failed: {r.status_code} {r.text}\") from ex\n","    return r.json()[\"access_token\"]\n","\n","\n","def _month_bounds_utc(day: datetime) -> tuple[str, str]:\n","    \"\"\"\n","    Return ISO 8601 timestamps (UTC) for the first and last moment of the month of 'day'.\n","    Example: ('2026-01-01T00:00:00Z', '2026-01-31T23:59:59Z')\n","    \"\"\"\n","    year, month = day.year, day.month\n","    last_day = monthrange(year, month)[1]\n","    start_iso = datetime(year, month, 1, 0, 0, 0, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","    end_iso   = datetime(year, month, last_day, 23, 59, 59, tzinfo=timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","    return start_iso, end_iso\n","\n","\n","def query_fabric_cost_daily_to_delta(\n","    tenant_id: str,\n","    client_id: str,\n","    client_secret: str,\n","    subscription_id: str,\n","    *,\n","    service_name_filter: str = \"Microsoft Fabric\",\n","    api_version: str = \"2019-11-01\",\n","    delta_table_name: str = \"stg_fabric_cost_daily\"\n","):\n","    \"\"\"\n","    Queries Azure Cost Management for the current month's daily cost filtered by ServiceName,\n","    expands columns using response metadata, and writes to a Delta table if Spark is present.\n","\n","    Returns:\n","        - If Spark available: writes to Delta and returns the Spark DataFrame.\n","        - Otherwise: returns a Pandas DataFrame.\n","    \"\"\"\n","\n","    # 1) Token\n","    access_token = _aad_client_credentials_token(tenant_id, client_id, client_secret)\n","    headers = {\"Authorization\": f\"Bearer {access_token}\", \"Content-Type\": \"application/json\"}\n","\n","    # 2) Time range: 'last date from the cost table until yesterday'\n","    start_iso = get_max_or_default(\"fabric_cost_daily\", \"date\")\n","    start_dt_utc = datetime.strptime(start_iso, \"%Y-%m-%d\").replace(tzinfo=timezone.utc)\n","    start_dt_utc = start_dt_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","    current_dt_utc = (datetime.utcnow()).replace(tzinfo=timezone.utc)\n","    _, end_dt_utc = _month_bounds_utc(current_dt_utc) \n","\n","    \n","    start_dt_utc = datetime.fromisoformat(start_dt_utc.replace(\"Z\", \"+00:00\"))\n","    end_dt_utc   = datetime.fromisoformat(end_dt_utc.replace(\"Z\", \"+00:00\"))\n","\n","    one_year = timedelta(days=180)\n","\n","    if (end_dt_utc - start_dt_utc) > one_year:\n","        start_dt_utc = end_dt_utc - one_year\n","\n","    # Only now, format to the timestamps you need downstream\n","    start_dt_iso = start_dt_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","    end_dt_iso   = end_dt_utc.strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n","\n","    print(\"Cost Data\")\n","    print(\"Start Date:\", start_dt_iso)\n","    print(\"End Date:\", end_dt_iso)\n","\n","    # 3) Cost Management Query endpoint (subscription scope)\n","    usage_url = (\n","        f\"https://management.azure.com/subscriptions/{subscription_id}\"\n","        f\"/providers/Microsoft.CostManagement/query?api-version={api_version}\"\n","    )\n","\n","    # 4) Query body (keep your aggregations, groupings & filter)\n","    usage_data = {\n","        \"type\": \"Usage\",\n","        \"timeframe\": \"Custom\",\n","        \"timePeriod\": {\"from\": start_dt_iso, \"to\": end_dt_iso},\n","        \"dataset\": {\n","            \"granularity\": \"Daily\",\n","            \"aggregation\": {\n","                \"totalCost\": {\"name\": \"Cost\", \"function\": \"Sum\"}\n","            },\n","            \"grouping\": [\n","                {\"type\": \"Dimension\", \"name\": \"ServiceName\"},\n","                {\"type\": \"Dimension\", \"name\": \"ResourceId\"}\n","            ],\n","            \"filter\": {\n","                \"dimensions\": {\n","                    \"name\": \"ServiceName\",\n","                    \"operator\": \"In\",\n","                    \"values\": [service_name_filter]\n","                }\n","            }\n","        }\n","    }\n","\n","    # 5) POST query\n","    r = requests.post(usage_url, headers=headers, json=usage_data, timeout=120)\n","    try:\n","        r.raise_for_status()\n","    except requests.HTTPError as ex:\n","        raise RuntimeError(f\"Cost query failed: {r.status_code} {r.text}\") from ex\n","\n","    payload = r.json()\n","    # The response typically includes: properties.columns and properties.rows\n","    props = payload.get(\"properties\", {})\n","    columns_meta = props.get(\"columns\", [])\n","    rows = props.get(\"rows\", [])\n","\n","    if not columns_meta or not rows:\n","        # Nothing returnedâ€”return an empty DF to keep caller logic simple\n","        empty_df = pd.DataFrame(columns=[c.get(\"name\", f\"col_{i}\") for i, c in enumerate(columns_meta)])\n","        if _HAS_SPARK:\n","            sdf = spark.createDataFrame(empty_df)  # noqa: F821\n","            sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n","            return sdf\n","        return empty_df\n","\n","    # 6) Build columns from metadata (prevents column-order bugs)\n","    col_names = [c.get(\"name\", f\"col_{i}\") for i, c in enumerate(columns_meta)]\n","    df = pd.DataFrame(rows, columns=col_names)\n","\n","    # Your original mapping was:\n","    # cost, date, devicename, resourceid, currency\n","    # But Cost Mgmt columns vary with aggregation & groupings;\n","    # we keep dynamic names, then derive helpful fields.\n","\n","    # Common columns expected from this query:\n","    # 'Cost', 'UsageDate', 'ServiceName', 'ResourceId', 'Currency'\n","    # Normalize where possible:\n","    # Cost\n","    if \"Cost\" in df.columns:\n","        df[\"cost\"] = df[\"Cost\"]\n","    # UsageDate (int YYYYMMDD) or stringâ€”normalize to datetime\n","    usage_date_col = next((c for c in df.columns if c.lower() in (\"usagedate\", \"date\")), None)\n","    if usage_date_col:\n","        df[\"date\"] = pd.to_datetime(df[usage_date_col].astype(str), format=\"%Y%m%d\", errors=\"coerce\")\n","    else:\n","        # fallback to the time window start for all rows if not present\n","        df[\"date\"] = pd.to_datetime(start_iso)\n","\n","    # ResourceId â†’ derive capacityname\n","    if \"ResourceId\" in df.columns:\n","        df[\"resourceid\"] = df[\"ResourceId\"]\n","        df[\"capacityname\"] = df[\"ResourceId\"].apply(\n","            lambda x: x.split(\"/capacities/\")[-1] if isinstance(x, str) and \"/capacities/\" in x else None\n","        )\n","    else:\n","        df[\"resourceid\"] = None\n","        df[\"capacityname\"] = None\n","\n","    # ServiceName â†’ devicename (keep your original target field name)\n","    if \"ServiceName\" in df.columns:\n","        df[\"devicename\"] = df[\"ServiceName\"]\n","    else:\n","        df[\"devicename\"] = service_name_filter\n","\n","    # Currency\n","    if \"Currency\" in df.columns:\n","        df[\"currency\"] = df[\"Currency\"]\n","    elif \"BillingCurrency\" in df.columns:\n","        df[\"currency\"] = df[\"BillingCurrency\"]\n","    else:\n","        df[\"currency\"] = None\n","\n","    # Final projection matching your original table intent\n","    final_cols = [\"cost\", \"date\", \"devicename\", \"resourceid\", \"currency\", \"capacityname\"]\n","    for c in final_cols:\n","        if c not in df.columns:\n","            df[c] = None\n","    df = df[final_cols]\n","\n","    # 7) Write to Delta if Spark available; else return pandas DF\n","    if _HAS_SPARK:\n","        sdf = spark.createDataFrame(df)  # noqa: F821\n","        sdf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(delta_table_name)\n","        return sdf\n","\n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7160137Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:40.4049515Z","execution_finish_time":"2026-02-04T05:49:40.6975388Z","parent_msg_id":"d09cf64e-08d9-478d-a672-00448684ea0d"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cf9819a8-bab8-4c61-a5e1-283f67cdae6b"},{"cell_type":"markdown","source":["**8.0 This code reads each stg_ table, fixes null/void columns, infers and casts column types from a sample, and deduplicates rows (keeping the latest per key when an order column is provided).**\n","\n","It then ensures a corresponding target table exists and MERGEs (upserts) the cleaned data into it using specified keys, optionally collecting record counts.\n","Finally, it returns a concise summary per table with keys, inferred types, void columns handled, and optional counts."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"f6a7162b-cc9d-4c5b-abc0-257b5fdefbac"},{"cell_type":"code","source":["\n","from pyspark.sql import functions as F\n","from pyspark.sql import Window\n","from pyspark.sql.types import StringType, NullType\n","\n","def process_stage_tables(\n","    *,\n","    spark_session=None,\n","    stage_tables=None,\n","    pk_map=None,\n","    latest_order_col_map=None,\n","    dedupe_partition_key_map=None,\n","    merge_keys_map=None,\n","    sample_size: int = 1000,\n","    db: str | None = None,\n","    create_missing_final: bool = True,\n","    set_confs: bool = True,\n","    collect_counts: bool = False,   # counts can be expensive; default False\n","    log_prefix: str = \"INGEST\"\n","):\n","    \"\"\"\n","    Ingest pattern: stage_* -> infer -> cast -> DEDUPE -> MERGE to final (Spark-only).\n","\n","    Parameters\n","    ----------\n","    spark_session : SparkSession, optional\n","        Provide an explicit SparkSession; otherwise uses global 'spark'.\n","    stage_tables : list[str]\n","        List of stage tables to process (e.g., [\"stg_items\", \"stg_dates\", ...]).\n","    pk_map : dict[str, list[str]]\n","        Map of table -> primary keys (physical or business keys present in both stage & final).\n","    latest_order_col_map : dict[str, str], optional\n","        Table -> column name that defines â€œlatestâ€ (DESC). If missing/None, we dropDuplicates by dedupe keys.\n","    dedupe_partition_key_map : dict[str, list[str]], optional\n","        Table -> partition keys for dedupe (defaults to pk_map[table] if not specified).\n","        Use this when your physical PK includes time but you want to dedupe on a business key.\n","    merge_keys_map : dict[str, list[str]], optional\n","        Table -> keys used in MERGE ON (defaults to pk_map[table] if not specified).\n","        Use this to upsert on a subset of columns (e.g., unique_key only).\n","    sample_size : int\n","        Number of rows to sample for string type inference.\n","    db : str | None\n","        Optional database/schema name; if provided, tables are read/written as f\"{db}.{table}\".\n","    create_missing_final : bool\n","        If a final table doesn't exist, create it from the deduped stage DF.\n","    set_confs : bool\n","        If True, sets configs to keep auto-merge OFF and use legacy time parser.\n","    collect_counts : bool\n","        If True, collect counts for raw/deduped records in the summary (may be costly).\n","    log_prefix : str\n","        Prefix used in prints for readability.\n","\n","    Returns\n","    -------\n","    list[dict]\n","        A summary per processed table with keys: table, final_table, merge_keys, void_cols, inferred_types, counts (optional).\n","    \"\"\"\n","\n","    # ----------------------------------------------------\n","    # 0) Resolve SparkSession and defaults\n","    # ----------------------------------------------------\n","    ss = spark_session or spark  # noqa: F821\n","\n","    stage_tables = stage_tables or []\n","    pk_map = pk_map or {}\n","    latest_order_col_map = latest_order_col_map or {}\n","    dedupe_partition_key_map = dedupe_partition_key_map or {}\n","    merge_keys_map = merge_keys_map or {}\n","\n","    if set_confs:\n","        # Keep auto-merge OFF; explicitly control schema/columns for MERGE\n","        ss.conf.set(\"spark.databricks.delta.schema.autoMerge.enabled\", \"false\")\n","        # Use legacy parser to avoid strict parse errors (matches your original)\n","        ss.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n","\n","    # ----------------------------------------------------\n","    # 1) Helpers (Spark-only)\n","    # ----------------------------------------------------\n","    TRUE_SET  = (\"true\", \"yes\", \"1\")\n","    FALSE_SET = (\"false\", \"no\", \"0\")\n","    BOOL_SET  = TRUE_SET + FALSE_SET\n","\n","    def infer_type_for_string_col(df_sample, col_name):\n","        \"\"\"\n","        Infer a type for a STRING column using Spark-only ops on df_sample.\n","        Returns: 'boolean' | 'long' | 'double' | 'timestamp' | 'date' | 'string'.\n","        Rule: ALL non-empty values must satisfy the target pattern.\n","        \"\"\"\n","        c = F.col(col_name)\n","        c_trim  = F.trim(c)\n","        c_lower = F.lower(c_trim)\n","\n","        non_empty_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\"), 1).otherwise(0)).alias(\"non_empty\")\n","        ).collect()[0][\"non_empty\"]\n","        if not non_empty_cnt or non_empty_cnt == 0:\n","            return \"string\"\n","\n","        bool_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\") & c_lower.isin(*BOOL_SET), 1).otherwise(0)).alias(\"b\")\n","        ).collect()[0][\"b\"]\n","        if bool_cnt == non_empty_cnt:\n","            return \"boolean\"\n","\n","        int_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\") & c_trim.rlike(r\"^-?\\d+$\"), 1).otherwise(0)).alias(\"i\")\n","        ).collect()[0][\"i\"]\n","        if int_cnt == non_empty_cnt:\n","            return \"long\"\n","\n","        float_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\") & c_trim.rlike(r\"^-?\\d+(?:\\.\\d+)?$\"), 1).otherwise(0)).alias(\"f\")\n","        ).collect()[0][\"f\"]\n","        if float_cnt == non_empty_cnt:\n","            return \"double\"\n","\n","        ts_parsed = F.coalesce(\n","            F.to_timestamp(c_trim, \"yyyy-MM-dd HH:mm:ss\"),\n","            F.to_timestamp(c_trim, \"yyyy-MM-dd'T'HH:mm:ss\"),\n","            F.to_timestamp(c_trim, \"MM/dd/yyyy HH:mm:ss\")\n","        )\n","        ts_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\") & ts_parsed.isNotNull(), 1).otherwise(0)).alias(\"t\")\n","        ).collect()[0][\"t\"]\n","        if ts_cnt == non_empty_cnt:\n","            return \"timestamp\"\n","\n","        d_parsed = F.coalesce(\n","            F.to_date(c_trim, \"yyyy-MM-dd\"),\n","            F.to_date(c_trim, \"MM/dd/yyyy\"),\n","            F.to_date(c_trim, \"yyyy/MM/dd\")\n","        )\n","        d_cnt = df_sample.select(\n","            F.sum(F.when(c.isNotNull() & (c_trim != \"\") & d_parsed.isNotNull(), 1).otherwise(0)).alias(\"d\")\n","        ).collect()[0][\"d\"]\n","        if d_cnt == non_empty_cnt:\n","            return \"date\"\n","\n","        return \"string\"\n","\n","    def cast_column(col_name, target_type):\n","        \"\"\"Apply casting logic to a column; always explicit for STRING.\"\"\"\n","        c = F.col(col_name)\n","        c_trim = F.trim(c)\n","\n","        if target_type == \"boolean\":\n","            return (\n","                F.when(F.lower(c_trim).isin(*TRUE_SET),  F.lit(True))\n","                 .when(F.lower(c_trim).isin(*FALSE_SET), F.lit(False))\n","                 .otherwise(F.lit(None))\n","            ).alias(col_name)\n","\n","        if target_type == \"long\":\n","            return c.cast(\"long\").alias(col_name)\n","\n","        if target_type == \"double\":\n","            return c.cast(\"double\").alias(col_name)\n","\n","        if target_type == \"date\":\n","            return F.coalesce(\n","                F.to_date(c, \"yyyy-MM-dd\"),\n","                F.to_date(c, \"MM/dd/yyyy\"),\n","                F.to_date(c, \"yyyy/MM/dd\")\n","            ).alias(col_name)\n","\n","        if target_type == \"timestamp\":\n","            return F.coalesce(\n","                F.to_timestamp(c, \"yyyy-MM-dd HH:mm:ss\"),\n","                F.to_timestamp(c, \"yyyy-MM-dd'T'HH:mm:ss\"),\n","                F.to_timestamp(c, \"MM/dd/yyyy HH:mm:ss\")\n","            ).alias(col_name)\n","\n","        # explicit string\n","        return F.trim(c).cast(StringType()).alias(col_name)\n","\n","    def keep_latest_per_key(df, partition_keys, order_col):\n","        \"\"\"\n","        Deduplicate df by partition_keys; keep latest row by order_col DESC.\n","        If order_col not provided/absent, falls back to dropDuplicates(partition_keys).\n","        \"\"\"\n","        if not order_col or order_col not in df.columns:\n","            print(f\"âš ï¸  [{log_prefix}] No valid 'latest' column; dropDuplicates({partition_keys}).\")\n","            return df.dropDuplicates(partition_keys)\n","\n","        w = Window.partitionBy([F.col(c) for c in partition_keys]).orderBy(F.col(order_col).desc())\n","        return (\n","            df.withColumn(\"_rn\", F.row_number().over(w))\n","              .filter(F.col(\"_rn\") == 1)\n","              .drop(\"_rn\")\n","        )\n","    \n","    \n","    def table_exists(fully_qualified_name: str) -> bool:\n","        \"\"\"\n","        Accepts 1-, 2-, or 3-part names:\n","        - \"table\"\n","        - \"db.table\"\n","        - \"catalog.schema.table\" (Unity Catalog / Fabric)\n","        \"\"\"\n","        try:\n","            return bool(spark.catalog.tableExists(fully_qualified_name))\n","        except Exception:\n","            # Defensive: tableExists usually won't throw, but be safe.\n","            return False\n","\n","    # ----------------------------------------------------\n","    # 2) Main loop\n","    # ----------------------------------------------------\n","    summaries = []\n","\n","    for stg in (stage_tables or []):\n","        print(f\"\\n=== [{log_prefix}] Processing: {stg} ===\")\n","\n","        \n","        if not table_exists(stg):\n","            print(f\"âš ï¸ Skipping '{stg}': table does not exist.\")\n","            continue\n","    \n","        if stg not in pk_map:\n","            raise ValueError(f\"[{log_prefix}] Missing PK in pk_map for {stg}\")\n","\n","        final_table = stg.replace(\"stg_\", \"\")\n","        pk_cols     = pk_map[stg]\n","        part_keys   = dedupe_partition_key_map.get(stg, pk_cols)\n","        order_col   = latest_order_col_map.get(stg)\n","        merge_keys  = merge_keys_map.get(stg, pk_cols)\n","\n","        # A) Read stage\n","        stage_fqn = f\"{db}.{stg}\" if db else stg\n","        df_raw = ss.read.table(stage_fqn)\n","\n","        # Force void/NullType to STRING\n","        void_cols = [\n","            f.name for f in df_raw.schema.fields\n","            if isinstance(f.dataType, NullType) or f.dataType.simpleString().lower() in (\"void\", \"null\")\n","        ]\n","        if void_cols:\n","            print(f\"[{log_prefix}] Force-casting void columns to STRING: {void_cols}\")\n","\n","        df_full = df_raw.select([\n","            F.trim(F.col(c)).cast(StringType()).alias(c) if c in void_cols else F.col(c).alias(c)\n","            for c in df_raw.columns\n","        ])\n","\n","        # B) Infer types for STRING columns on sample\n","        df_sample = df_full.limit(sample_size)\n","        string_cols = [f.name for f in df_sample.schema.fields if f.dataType.simpleString() == \"string\"]\n","\n","        inferred = {}\n","        for c in string_cols:\n","            inferred[c] = infer_type_for_string_col(df_sample, c)\n","        print(f\"[{log_prefix}] Inferred string types: {inferred}\")\n","\n","        # C) Cast full DF\n","        df_cast = df_full.select([\n","            cast_column(c, inferred[c]) if c in inferred else F.col(c).alias(c)\n","            for c in df_full.columns\n","        ])\n","\n","        print(f\"[{log_prefix}] Stage schema (no void expected):\")\n","\n","        # df_cast.printSchema()\n","\n","        # D) DEDUPE\n","        # safety checks\n","        for k in part_keys:\n","            if k not in df_cast.columns:\n","                raise ValueError(f\"[{log_prefix}] Partition key '{k}' missing in {stg}; cols: {df_cast.columns}\")\n","        if order_col and order_col not in df_cast.columns:\n","            raise ValueError(f\"[{log_prefix}] Order-by column '{order_col}' missing in {stg}; cols: {df_cast.columns}\")\n","\n","        df_latest = keep_latest_per_key(df_cast, part_keys, order_col)\n","\n","        # E) Ensure final exists\n","        final_fqn = f\"{db}.{final_table}\" if db else final_table\n","        if not ss.catalog.tableExists(final_fqn):\n","            if create_missing_final:\n","                print(f\"[{log_prefix}] Creating final table: {final_fqn}\")\n","                df_latest.write.mode(\"overwrite\").saveAsTable(final_fqn)\n","            else:\n","                raise ValueError(f\"[{log_prefix}] Final table does not exist: {final_fqn}\")\n","\n","        # F) MERGE only common columns\n","        final_df   = ss.read.table(final_fqn)\n","        final_cols = final_df.columns\n","        common     = [c for c in df_latest.columns if c in final_cols]\n","\n","        if not common:\n","            print(f\"[{log_prefix}] WARNING: No common columns between {stg} and {final_fqn}; skipping.\")\n","            summaries.append({\n","                \"table\": stg, \"final_table\": final_fqn, \"merge_keys\": merge_keys,\n","                \"void_cols\": void_cols, \"inferred_types\": inferred, \"skipped\": True\n","            })\n","            continue\n","\n","        # Temp views must be unique per table to avoid clashes\n","        tv_final = f\"final_view_{final_table}\"\n","        tv_stage = f\"stage_view_{final_table}\"\n","        final_df.select(common).createOrReplaceTempView(tv_final)\n","        df_latest.select(common).createOrReplaceTempView(tv_stage)\n","\n","        # ON clause\n","        on_pairs = [f\"f.`{pk}` = s.`{pk}`\" for pk in merge_keys if pk in common]\n","        missing_pks = [pk for pk in merge_keys if pk not in common]\n","        if missing_pks:\n","            raise ValueError(f\"[{log_prefix}] MERGE key(s) {missing_pks} not in common columns for {stg} -> {final_fqn}\")\n","\n","        on_clause = \" AND \".join(on_pairs)\n","        non_pk    = [c for c in common if c not in merge_keys]\n","        set_clause  = \", \".join([f\"f.`{c}` = s.`{c}`\" for c in non_pk]) if non_pk else \"\"\n","        insert_cols = \", \".join([f\"`{c}`\" for c in common])\n","        insert_vals = \", \".join([f\"s.`{c}`\" for c in common])\n","\n","        merge_sql = f\"\"\"\n","        MERGE INTO {final_fqn} AS f\n","        USING {tv_stage} AS s\n","        ON {on_clause}\n","        {\"WHEN MATCHED THEN UPDATE SET \" + set_clause if set_clause else \"\"}\n","        WHEN NOT MATCHED THEN INSERT ({insert_cols}) VALUES ({insert_vals})\n","        \"\"\"\n","\n","        # print(merge_sql)\n","        ss.sql(merge_sql)\n","\n","        print(f\"[{log_prefix}] Upserted {stg} -> {final_fqn} on keys {merge_keys}\")\n","\n","        # Optional counts (actions)\n","        counts = {}\n","        if collect_counts:\n","            try:\n","                counts[\"stage_raw\"]   = df_raw.count()\n","                counts[\"stage_latest\"] = df_latest.count()\n","                counts[\"final_after\"] = ss.read.table(final_fqn).count()\n","            except Exception as _:\n","                counts[\"error\"] = \"count_failed\"\n","\n","        summaries.append({\n","            \"table\": stg,\n","            \"final_table\": final_fqn,\n","            \"merge_keys\": merge_keys,\n","            \"void_cols\": void_cols,\n","            \"inferred_types\": inferred,\n","            \"counts\": counts if counts else None\n","        })\n","\n","    return summaries\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7260096Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:40.7000666Z","execution_finish_time":"2026-02-04T05:49:41.0239994Z","parent_msg_id":"9c0c861b-252e-4c87-9a35-3605e7690b2c"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11ca6174-4425-4f55-953c-497306f0b0d1"},{"cell_type":"markdown","source":["**9.0 This function runs the full ingestion pipeline by defining all stage tables and their keys, then calling process_stage_tables() to clean, dedupe, type-cast, and merge each stage table into its final table.**\n","\n","After processing, it prints a summary of what happened for each table."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"f03029bf-065d-457b-abb0-b315459a51db"},{"cell_type":"code","source":["def merge_stage_data():\n","\n","        # 0) Your configuration (same shape as in your notebook)\n","        stage_tables = [\n","            \"stg_capacities\",\n","            \"stg_dates\",\n","            \"stg_workloads\",\n","            \"stg_workspaces\",\n","            \"stg_metrics_by_item_and_day\",\n","            \"stg_items\",\n","            \"stg_fabric_cost_daily\"\n","        ]\n","\n","        pk_map = {\n","            \"stg_capacities\": [\"capacities_capacity_id\"],\n","            \"stg_dates\": [\"dates_date\"],\n","            \"stg_workloads\": [\"workloads_workload_kind\"],\n","            \"stg_workspaces\": [\"workspaces_workspace_id\"],\n","            \"stg_metrics_by_item_and_day\": [\"metrics_by_item_and_day_unique_key\", \"metrics_by_item_and_day_datetime\"],\n","            \"stg_items\": [\"items_item_id\"],\n","            \"stg_fabric_cost_daily\": [\"date\", \"capacityname\"],\n","        }\n","\n","        latest_order_col_map = {\n","            # \"stg_metrics_by_item_and_day\": \"metrics_by_item_and_day_datetime\",\n","            \"stg_items\": \"items_timestamp\",\n","        }\n","\n","        dedupe_partition_key_map = {\n","            # \"stg_metrics_by_item_and_day\": [\"metrics_by_item_and_day_unique_key\"],\n","        }\n","\n","        merge_keys_map = {\n","            # \"stg_metrics_by_item_and_day\": [\"mstg_metrics_by_item_and_day$0costetrics_by_item_and_day_unique_key\"],\n","        }\n","\n","        # 1) Run it\n","        summary = process_stage_tables(\n","            stage_tables=stage_tables,\n","            pk_map=pk_map,\n","            latest_order_col_map=latest_order_col_map,\n","            dedupe_partition_key_map=dedupe_partition_key_map,\n","            merge_keys_map=merge_keys_map,\n","            sample_size=1000,\n","            db=None,                     # or your database/schema name\n","            create_missing_final=True,\n","            set_confs=True,\n","            collect_counts=False,        # enable if you need counts\n","            log_prefix=\"PIPELINE\"\n","        )\n","\n","        # 2) Inspect result\n","        for s in summary:\n","            print(s)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":16,"statement_ids":[16],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7277108Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:41.0260003Z","execution_finish_time":"2026-02-04T05:49:41.3676585Z","parent_msg_id":"a84bb540-39e1-4a17-a7b8-293d9781154a"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 16, Finished, Available, Finished)"},"metadata":{}}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d819143f-4abd-4b40-9fd3-aa120572e212"},{"cell_type":"markdown","source":["**Main workflow starts here**\n","\n","1. Get the service principal details from the Key Vault.\n","2. Get the Metrics app workspace details and the dataset details.\n","3. Extract Fabric cost data.\n","4. For each capacity in the tenant, extract and process the capacity metrics data."],"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"nteract":{"transient":{"deleting":false}}},"id":"0691681e-16e5-4f1e-997f-10997adff30e"},{"cell_type":"code","source":["# Extracts tenantid, subscriptionid, appid,appsecret from the Key Vault\n","secrets  = get_kv_secrets(\n","    kv_name=\"kvrbacpocsk\",\n","    secret_names=[\"tenantid\", \"subscriptionid\", \"appid\", \"appsecret\"]\n",")\n","\n","tenant_id = secrets[\"tenantid\"]\n","subscriptionid = secrets[\"subscriptionid\"]\n","client_id = secrets[\"appid\"]\n","client_secret = secrets[\"appsecret\"]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7297873Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:41.3695746Z","execution_finish_time":"2026-02-04T05:49:43.5570398Z","parent_msg_id":"68f4ddd8-9634-476f-ac10-6b3c3c2c9577"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"477a29bd-e4bc-4cdf-9c29-f51dcebe77d4"},{"cell_type":"code","source":["# Resolve Capcity Metrics workspace details\n","# All functions now reuse the same fabric context\n","metrics_ws_name, metrics_ws_id, metrics_ds_name, metrics_ds_id = extract_metrics_workspace_details()\n","print(metrics_ws_name, metrics_ws_id, metrics_ds_name, metrics_ds_id)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":18,"statement_ids":[18],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7316662Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:43.5590208Z","execution_finish_time":"2026-02-04T05:49:49.4466893Z","parent_msg_id":"3ea906c6-8726-461c-89e3-48150dca9c83"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 18, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Microsoft Fabric Capacity Metrics 4bdee50b-585a-489a-bf6a-13ad81fdaaa4 Fabric Capacity Metrics 54f74ce5-da8c-4d87-91b3-f6550234856a\n"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d3f4c16-9a1c-4666-86b5-62ed660cc88f"},{"cell_type":"code","source":["# Extract Fabric cost data\n","df_or_sdf = query_fabric_cost_daily_to_delta(\n","    tenant_id=tenant_id,\n","    client_id=client_id,\n","    client_secret=client_secret,\n","    subscription_id=subscriptionid,\n","    service_name_filter=\"Microsoft Fabric\",   # change if needed\n","    delta_table_name=\"stg_fabric_cost_daily\"\n",")\n","# display(df_or_sdf)  # works for Spark DF; for pandas, print(df_or_sdf.head())"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":19,"statement_ids":[19],"state":"finished","livy_statement_state":"available","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"finished","queued_time":"2026-02-04T05:46:15.7349848Z","session_start_time":null,"execution_start_time":"2026-02-04T05:49:49.4485899Z","execution_finish_time":"2026-02-04T05:50:49.3468059Z","parent_msg_id":"d68a91a0-5ea3-4c73-bc08-9e0a060b67ba"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 19, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Cost Data\nStart Date: 2026-01-27T00:00:00Z\nEnd Date: 2026-02-28T23:59:59Z\n"]}],"execution_count":17,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5968947a-8292-404e-bd79-dae3d3e4e3c9"},{"cell_type":"code","source":["# Get all capcities in the tenant and for each capacity extarct the details from the metrics app into stage tables \n","# and then merage the stage data into target tables\n","\n","import sempy.fabric as fabric\n","from sempy.fabric.exceptions import FabricHTTPException\n","from notebookutils import notebook\n","\n","with set_service_principal(tenant_id=tenant_id, client_id=client_id, client_secret=client_secret):\n","     client = fabric.FabricRestClient()\n","url  = f\"https://api.powerbi.com/v1.0/myorg/capacities\"\n","fabric_rest_client = fabric.FabricRestClient()\n","capacities_json = fabric_rest_client.get(url)\n","# get a list of all capacities\n","capacities = capacities_json.json()[\"value\"]\n","\n","# process data from each capacity\n","for cap in capacities:\n","    region_name = cap.get(\"region\", \"Unknown Region\")\n","    capacity_id_to_extract_data_from = cap.get(\"id\", \"000000\")\n","    \n","    print(\"######### processing data #############\")\n","    print(\"processing the data for the capacity :\", f\"{capacity_id_to_extract_data_from}\") \n","    print(\"######### processing data #############\")\n","\n","    # update metrics dataset \n","    update_metrics_dataset(metrics_workspace_id = metrics_ws_id, metrics_dataset_id = metrics_ds_id, capacity_id_to_extract_data_from = capacity_id_to_extract_data_from, region_name = region_name)\n","\n","    # list of table to be processed\n","    table_list = [\n","            \"Capacities\",\n","            \"Dates\",\n","            \"Metrics By Item and Day\",\n","            \"Workloads\",\n","            \"Workspaces\",\n","            \"Items\",\n","            \"Metrics By Item\"\n","            ]\n","    for table_name in table_list:\n","            print(\"processing the table:\", table_name)      \n","            # extract data for each one of the tables from metrics dataset\n","            get_table_data(metrics_workspace_id = metrics_ws_id, metrics_dataset_id = metrics_ds_id, table_name= table_name, capacity_id = capacity_id_to_extract_data_from)\n","    # merge stage data into target tables            \n","    merge_stage_data()\n","\n","#     if bool(spark.catalog.tableExists(\"metrics_by_item_and_day\")) and bool(spark.catalog.tableExists(\"stg_metrics_by_item_and_day\")):\n","#        display(spark.sql(\"select distinct metrics_by_item_and_day_capacity_id FROM Metrics.metrics_by_item_and_day union select distinct metrics_by_item_and_day_capacity_id FROM Metrics.stg_metrics_by_item_and_day\"))"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":20,"statement_ids":[20],"state":"submitted","livy_statement_state":"running","session_id":"bad056f4-0aa5-4743-9663-648699cc5fa4","normalized_state":"running","queued_time":"2026-02-04T05:46:15.739124Z","session_start_time":null,"execution_start_time":"2026-02-04T05:50:49.3492331Z","execution_finish_time":null,"parent_msg_id":"f0bb6fcd-61b9-4a3c-abc9-f968828c6edc"},"text/plain":"StatementMeta(, bad056f4-0aa5-4743-9663-648699cc5fa4, 20, Submitted, Running, Running)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["######### processing data #############\nprocessing the data for the capacity : 6A3D303C-56B7-4D7C-869D-1FBFC4D539CB\n######### processing data #############\nâœ… Parameters updated successfully.\nðŸ”„ Triggering dataset refreshâ€¦\n[0] status=Unknown\n[1] status=Unknown\n[2] status=Unknown\n[3] status=Unknown\n[4] status=Completed\nRefresh succeeded.\nprocessing the table: Capacities\nprocessing the table: Dates\nprocessing the table: Metrics By Item and Day\ngetting the data from start_iso = 2026-01-28\nAn unexpected error occurred: list index out of range\nprocessing the table: Workloads\nprocessing the table: Workspaces\nprocessing the table: Items\nprocessing the table: Metrics By Item\nAn unexpected error occurred: list index out of range\n\n=== [PIPELINE] Processing: stg_capacities ===\n[PIPELINE] Inferred string types: {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['capacities_capacity_id']).\n[PIPELINE] Upserted stg_capacities -> capacities on keys ['capacities_capacity_id']\n\n=== [PIPELINE] Processing: stg_dates ===\n[PIPELINE] Inferred string types: {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['dates_date']).\n[PIPELINE] Upserted stg_dates -> dates on keys ['dates_date']\n\n=== [PIPELINE] Processing: stg_workloads ===\n[PIPELINE] Inferred string types: {'workloads_workload_kind': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workloads_workload_kind']).\n[PIPELINE] Upserted stg_workloads -> workloads on keys ['workloads_workload_kind']\n\n=== [PIPELINE] Processing: stg_workspaces ===\n[PIPELINE] Inferred string types: {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workspaces_workspace_id']).\n[PIPELINE] Upserted stg_workspaces -> workspaces on keys ['workspaces_workspace_id']\n\n=== [PIPELINE] Processing: stg_metrics_by_item_and_day ===\n[PIPELINE] Inferred string types: {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']).\n[PIPELINE] Upserted stg_metrics_by_item_and_day -> metrics_by_item_and_day on keys ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']\n\n=== [PIPELINE] Processing: stg_items ===\n[PIPELINE] Inferred string types: {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\n[PIPELINE] Upserted stg_items -> items on keys ['items_item_id']\n\n=== [PIPELINE] Processing: stg_fabric_cost_daily ===\n[PIPELINE] Inferred string types: {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['date', 'capacityname']).\n[PIPELINE] Upserted stg_fabric_cost_daily -> fabric_cost_daily on keys ['date', 'capacityname']\n{'table': 'stg_capacities', 'final_table': 'capacities', 'merge_keys': ['capacities_capacity_id'], 'void_cols': [], 'inferred_types': {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}, 'counts': None}\n{'table': 'stg_dates', 'final_table': 'dates', 'merge_keys': ['dates_date'], 'void_cols': [], 'inferred_types': {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}, 'counts': None}\n{'table': 'stg_workloads', 'final_table': 'workloads', 'merge_keys': ['workloads_workload_kind'], 'void_cols': [], 'inferred_types': {'workloads_workload_kind': 'string'}, 'counts': None}\n{'table': 'stg_workspaces', 'final_table': 'workspaces', 'merge_keys': ['workspaces_workspace_id'], 'void_cols': [], 'inferred_types': {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}, 'counts': None}\n{'table': 'stg_metrics_by_item_and_day', 'final_table': 'metrics_by_item_and_day', 'merge_keys': ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime'], 'void_cols': [], 'inferred_types': {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}, 'counts': None}\n{'table': 'stg_items', 'final_table': 'items', 'merge_keys': ['items_item_id'], 'void_cols': [], 'inferred_types': {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}, 'counts': None}\n{'table': 'stg_fabric_cost_daily', 'final_table': 'fabric_cost_daily', 'merge_keys': ['date', 'capacityname'], 'void_cols': [], 'inferred_types': {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}, 'counts': None}\n######### processing data #############\nprocessing the data for the capacity : ECF6CD6D-1DA7-479D-9C50-CB4A6264824A\n######### processing data #############\nâœ… Parameters updated successfully.\nðŸ”„ Triggering dataset refreshâ€¦\n[0] status=Unknown\n[1] status=Unknown\n[2] status=Unknown\n[3] status=Unknown\n[4] status=Completed\nRefresh succeeded.\nprocessing the table: Capacities\nprocessing the table: Dates\nprocessing the table: Metrics By Item and Day\ngetting the data from start_iso = 2026-01-28\nAn unexpected error occurred: list index out of range\nprocessing the table: Workloads\nprocessing the table: Workspaces\nprocessing the table: Items\nprocessing the table: Metrics By Item\nAn unexpected error occurred: list index out of range\n\n=== [PIPELINE] Processing: stg_capacities ===\n[PIPELINE] Inferred string types: {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['capacities_capacity_id']).\n[PIPELINE] Upserted stg_capacities -> capacities on keys ['capacities_capacity_id']\n\n=== [PIPELINE] Processing: stg_dates ===\n[PIPELINE] Inferred string types: {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['dates_date']).\n[PIPELINE] Upserted stg_dates -> dates on keys ['dates_date']\n\n=== [PIPELINE] Processing: stg_workloads ===\n[PIPELINE] Inferred string types: {'workloads_workload_kind': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workloads_workload_kind']).\n[PIPELINE] Upserted stg_workloads -> workloads on keys ['workloads_workload_kind']\n\n=== [PIPELINE] Processing: stg_workspaces ===\n[PIPELINE] Inferred string types: {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workspaces_workspace_id']).\n[PIPELINE] Upserted stg_workspaces -> workspaces on keys ['workspaces_workspace_id']\n\n=== [PIPELINE] Processing: stg_metrics_by_item_and_day ===\n[PIPELINE] Inferred string types: {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']).\n[PIPELINE] Upserted stg_metrics_by_item_and_day -> metrics_by_item_and_day on keys ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']\n\n=== [PIPELINE] Processing: stg_items ===\n[PIPELINE] Inferred string types: {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\n[PIPELINE] Upserted stg_items -> items on keys ['items_item_id']\n\n=== [PIPELINE] Processing: stg_fabric_cost_daily ===\n[PIPELINE] Inferred string types: {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['date', 'capacityname']).\n[PIPELINE] Upserted stg_fabric_cost_daily -> fabric_cost_daily on keys ['date', 'capacityname']\n{'table': 'stg_capacities', 'final_table': 'capacities', 'merge_keys': ['capacities_capacity_id'], 'void_cols': [], 'inferred_types': {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}, 'counts': None}\n{'table': 'stg_dates', 'final_table': 'dates', 'merge_keys': ['dates_date'], 'void_cols': [], 'inferred_types': {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}, 'counts': None}\n{'table': 'stg_workloads', 'final_table': 'workloads', 'merge_keys': ['workloads_workload_kind'], 'void_cols': [], 'inferred_types': {'workloads_workload_kind': 'string'}, 'counts': None}\n{'table': 'stg_workspaces', 'final_table': 'workspaces', 'merge_keys': ['workspaces_workspace_id'], 'void_cols': [], 'inferred_types': {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}, 'counts': None}\n{'table': 'stg_metrics_by_item_and_day', 'final_table': 'metrics_by_item_and_day', 'merge_keys': ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime'], 'void_cols': [], 'inferred_types': {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}, 'counts': None}\n{'table': 'stg_items', 'final_table': 'items', 'merge_keys': ['items_item_id'], 'void_cols': [], 'inferred_types': {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}, 'counts': None}\n{'table': 'stg_fabric_cost_daily', 'final_table': 'fabric_cost_daily', 'merge_keys': ['date', 'capacityname'], 'void_cols': [], 'inferred_types': {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}, 'counts': None}\n######### processing data #############\nprocessing the data for the capacity : 31E11722-244B-49DC-B491-7D617D0E905B\n######### processing data #############\nâœ… Parameters updated successfully.\nðŸ”„ Triggering dataset refreshâ€¦\n[0] status=Unknown\n[1] status=Unknown\n[2] status=Unknown\n[3] status=Completed\nRefresh succeeded.\nprocessing the table: Capacities\nprocessing the table: Dates\nprocessing the table: Metrics By Item and Day\ngetting the data from start_iso = 2026-01-28\nprocessing the table: Workloads\nprocessing the table: Workspaces\nprocessing the table: Items\nprocessing the table: Metrics By Item\n\n=== [PIPELINE] Processing: stg_capacities ===\n[PIPELINE] Inferred string types: {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['capacities_capacity_id']).\n[PIPELINE] Upserted stg_capacities -> capacities on keys ['capacities_capacity_id']\n\n=== [PIPELINE] Processing: stg_dates ===\n[PIPELINE] Inferred string types: {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['dates_date']).\n[PIPELINE] Upserted stg_dates -> dates on keys ['dates_date']\n\n=== [PIPELINE] Processing: stg_workloads ===\n[PIPELINE] Inferred string types: {'workloads_workload_kind': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workloads_workload_kind']).\n[PIPELINE] Upserted stg_workloads -> workloads on keys ['workloads_workload_kind']\n\n=== [PIPELINE] Processing: stg_workspaces ===\n[PIPELINE] Inferred string types: {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workspaces_workspace_id']).\n[PIPELINE] Upserted stg_workspaces -> workspaces on keys ['workspaces_workspace_id']\n\n=== [PIPELINE] Processing: stg_metrics_by_item_and_day ===\n[PIPELINE] Inferred string types: {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']).\n[PIPELINE] Upserted stg_metrics_by_item_and_day -> metrics_by_item_and_day on keys ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']\n\n=== [PIPELINE] Processing: stg_items ===\n[PIPELINE] Inferred string types: {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\n[PIPELINE] Upserted stg_items -> items on keys ['items_item_id']\n\n=== [PIPELINE] Processing: stg_fabric_cost_daily ===\n[PIPELINE] Inferred string types: {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['date', 'capacityname']).\n[PIPELINE] Upserted stg_fabric_cost_daily -> fabric_cost_daily on keys ['date', 'capacityname']\n{'table': 'stg_capacities', 'final_table': 'capacities', 'merge_keys': ['capacities_capacity_id'], 'void_cols': [], 'inferred_types': {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}, 'counts': None}\n{'table': 'stg_dates', 'final_table': 'dates', 'merge_keys': ['dates_date'], 'void_cols': [], 'inferred_types': {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}, 'counts': None}\n{'table': 'stg_workloads', 'final_table': 'workloads', 'merge_keys': ['workloads_workload_kind'], 'void_cols': [], 'inferred_types': {'workloads_workload_kind': 'string'}, 'counts': None}\n{'table': 'stg_workspaces', 'final_table': 'workspaces', 'merge_keys': ['workspaces_workspace_id'], 'void_cols': [], 'inferred_types': {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}, 'counts': None}\n{'table': 'stg_metrics_by_item_and_day', 'final_table': 'metrics_by_item_and_day', 'merge_keys': ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime'], 'void_cols': [], 'inferred_types': {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}, 'counts': None}\n{'table': 'stg_items', 'final_table': 'items', 'merge_keys': ['items_item_id'], 'void_cols': [], 'inferred_types': {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}, 'counts': None}\n{'table': 'stg_fabric_cost_daily', 'final_table': 'fabric_cost_daily', 'merge_keys': ['date', 'capacityname'], 'void_cols': [], 'inferred_types': {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}, 'counts': None}\n######### processing data #############\nprocessing the data for the capacity : 785262F9-E588-40BA-986E-3E12FFE3CCC2\n######### processing data #############\nâœ… Parameters updated successfully.\nðŸ”„ Triggering dataset refreshâ€¦\n[0] status=Unknown\n[1] status=Unknown\n[2] status=Unknown\n[3] status=Unknown\n[4] status=Completed\nRefresh succeeded.\nprocessing the table: Capacities\nprocessing the table: Dates\nprocessing the table: Metrics By Item and Day\ngetting the data from start_iso = 2026-01-28\nprocessing the table: Workloads\nprocessing the table: Workspaces\nprocessing the table: Items\nprocessing the table: Metrics By Item\n\n=== [PIPELINE] Processing: stg_capacities ===\n[PIPELINE] Inferred string types: {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['capacities_capacity_id']).\n[PIPELINE] Upserted stg_capacities -> capacities on keys ['capacities_capacity_id']\n\n=== [PIPELINE] Processing: stg_dates ===\n[PIPELINE] Inferred string types: {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['dates_date']).\n[PIPELINE] Upserted stg_dates -> dates on keys ['dates_date']\n\n=== [PIPELINE] Processing: stg_workloads ===\n[PIPELINE] Inferred string types: {'workloads_workload_kind': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workloads_workload_kind']).\n[PIPELINE] Upserted stg_workloads -> workloads on keys ['workloads_workload_kind']\n\n=== [PIPELINE] Processing: stg_workspaces ===\n[PIPELINE] Inferred string types: {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['workspaces_workspace_id']).\n[PIPELINE] Upserted stg_workspaces -> workspaces on keys ['workspaces_workspace_id']\n\n=== [PIPELINE] Processing: stg_metrics_by_item_and_day ===\n[PIPELINE] Inferred string types: {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']).\n[PIPELINE] Upserted stg_metrics_by_item_and_day -> metrics_by_item_and_day on keys ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime']\n\n=== [PIPELINE] Processing: stg_items ===\n[PIPELINE] Inferred string types: {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}\n[PIPELINE] Stage schema (no void expected):\n[PIPELINE] Upserted stg_items -> items on keys ['items_item_id']\n\n=== [PIPELINE] Processing: stg_fabric_cost_daily ===\n[PIPELINE] Inferred string types: {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}\n[PIPELINE] Stage schema (no void expected):\nâš ï¸  [PIPELINE] No valid 'latest' column; dropDuplicates(['date', 'capacityname']).\n[PIPELINE] Upserted stg_fabric_cost_daily -> fabric_cost_daily on keys ['date', 'capacityname']\n{'table': 'stg_capacities', 'final_table': 'capacities', 'merge_keys': ['capacities_capacity_id'], 'void_cols': [], 'inferred_types': {'capacities_capacity_id': 'string', 'capacities_state': 'string', 'capacities_region_without_default': 'string', 'capacities_capacity_name': 'string', 'capacities_sku': 'string', 'capacities_owners': 'string', 'capacities_region': 'string', 'capacities_uppercase_capacity_id': 'string'}, 'counts': None}\n{'table': 'stg_dates', 'final_table': 'dates', 'merge_keys': ['dates_date'], 'void_cols': [], 'inferred_types': {'dates_date': 'timestamp', 'dates_start_of_month': 'date', 'dates_first_day_of_week': 'timestamp', 'dates_day': 'timestamp'}, 'counts': None}\n{'table': 'stg_workloads', 'final_table': 'workloads', 'merge_keys': ['workloads_workload_kind'], 'void_cols': [], 'inferred_types': {'workloads_workload_kind': 'string'}, 'counts': None}\n{'table': 'stg_workspaces', 'final_table': 'workspaces', 'merge_keys': ['workspaces_workspace_id'], 'void_cols': [], 'inferred_types': {'workspaces_workspace_id': 'string', 'workspaces_workspace_key': 'string', 'workspaces_workspace_name': 'string', 'workspaces_capacity_id': 'string', 'workspaces_workspace_provision_state': 'string'}, 'counts': None}\n{'table': 'stg_metrics_by_item_and_day', 'final_table': 'metrics_by_item_and_day', 'merge_keys': ['metrics_by_item_and_day_unique_key', 'metrics_by_item_and_day_datetime'], 'void_cols': [], 'inferred_types': {'metrics_by_item_and_day_datetime': 'timestamp', 'metrics_by_item_and_day_date': 'timestamp', 'metrics_by_item_and_day_capacity_id': 'string', 'metrics_by_item_and_day_item_id': 'string', 'metrics_by_item_and_day_workspace_id': 'string', 'metrics_by_item_and_day_unique_key': 'string'}, 'counts': None}\n{'table': 'stg_items', 'final_table': 'items', 'merge_keys': ['items_item_id'], 'void_cols': [], 'inferred_types': {'items_capacity_id': 'string', 'items_item_id': 'string', 'items_item_kind': 'string', 'items_item_name': 'string', 'items_users': 'string', 'items_timestamp': 'timestamp', 'items_workspace_id': 'string', 'items_workspace_name': 'string', 'items_billable_type': 'string', 'items_virtualised_item': 'boolean', 'items_virtualised_workspace': 'boolean', 'items_is_virtual__item_status': 'boolean', 'items_is_virtual_workspace_status': 'boolean', 'items_unique_key': 'string', 'items_item_key': 'string'}, 'counts': None}\n{'table': 'stg_fabric_cost_daily', 'final_table': 'fabric_cost_daily', 'merge_keys': ['date', 'capacityname'], 'void_cols': [], 'inferred_types': {'devicename': 'string', 'resourceid': 'string', 'currency': 'string', 'capacityname': 'string'}, 'counts': None}\n######### processing data #############\nprocessing the data for the capacity : 5420CA3B-55F2-4F55-987E-18333D594DE1\n######### processing data #############\nâœ… Parameters updated successfully.\nðŸ”„ Triggering dataset refreshâ€¦\n[0] status=Unknown\n[1] status=Unknown\n[2] status=Unknown\n[3] status=Completed\nRefresh succeeded.\nprocessing the table: Capacities\nprocessing the table: Dates\nprocessing the table: Metrics By Item and Day\ngetting the data from start_iso = 2026-01-28\n"]}],"execution_count":18,"metadata":{"collapsed":false,"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50e48cbd-715b-4935-bd44-3c06034626b6"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"display_name":"Synapse PySpark","language":"Python","name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"state":{},"version":"0.1"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"c2ab4516-db37-4cb2-896d-f0e2cc7f14e3","default_lakehouse_name":"Metrics","default_lakehouse_workspace_id":"1cfac5bb-d53c-4481-aaf4-f931fea01c1d","known_lakehouses":[{"id":"c2ab4516-db37-4cb2-896d-f0e2cc7f14e3"}]}}},"nbformat":4,"nbformat_minor":5}